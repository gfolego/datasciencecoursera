---
title: "Practical Machine Learning - Prediction Assignment"
author: "Guilherme Folego"
date: "2020-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=TRUE)
```

## Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


## Analysis

#### Enviroment

First, we need to load the necessary libraries and prepare the analysis enviroment. Here, for reproducibility, we also set a specific seed.

```{r libenv, message=FALSE}
library(caret)
library(doParallel)

cl <- makeCluster(detectCores())
registerDoParallel(cl)

set.seed(123456)
```


#### Load data

Then, we need to load the data for this assignment. One CSV file is for our analysis, and another one is for the final quiz.
We specify some parameters to optimize the CSV reading, as well as setting NAs to zero, to improve our pre-processing step later.

It is important to note that we ignore the first seven columns, since they are not actual accelerometer measurements, and, as such, they are not of interest. The last column represents our output classes.


```{r load}
urlDataALl  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlDataQuiz <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

colClasses <- c(rep("NULL", 7), rep(NA, 152), "factor")
naStrings <- c("NA", "#DIV/0!")

dataAll  <- read.csv(url(urlDataALl),  colClasses=colClasses, na.strings=naStrings)
dataQuiz <- read.csv(url(urlDataQuiz), colClasses=colClasses, na.strings=naStrings)

dataAll[is.na(dataAll)] <- 0
dataQuiz[is.na(dataQuiz)] <- 0

dim(dataAll)
dim(dataQuiz)
```

We can see that initially there are `r ncol(dataAll)-1` variables, together with our output class.

#### Split data

Now we split our analysis data into training and testing sets, with 70% for training. We will use our testing set only at the very end, to assess model performance.

```{r split}
inTrain <- createDataPartition(dataAll$classe, p=0.7, list=FALSE)
dataTraining <- dataAll[inTrain,]
dataTesting  <- dataAll[-inTrain,]

dim(dataTraining)
dim(dataTesting)
```


#### Pre-processing

Now we prepare the data for modeling. In this pre-processing step, we first remove variables with near zero variance, which also helps dealing with previous NA columns. Then, we center (zero mean), and scale (unit variance) the remaining variables.

```{r preproc}
preProc <- preProcess(dataTraining, method=c("nzv", "center", "scale"))
dataTraining <- predict(preProc, dataTraining)
dataTesting  <- predict(preProc, dataTesting)

dim(dataTraining)
dim(dataTesting)
```

We can see that now there are only `r ncol(dataTraining)-1` variables, together with our output class.

#### Model

We opted to train a random forest model, for both performance and interpretability.
For faster optimization, without loss of generalization, we estimated our performance during training using an out-of-bag (OOB) approach, instead of boosting or cross-validation.


```{r model}
model <- train(classe ~ .,
               data=dataTraining,
               method="rf",
               trControl=trainControl(method="oob"))
model
model$finalModel
```

```{r modelplot}
plot(model$finalModel, main="OOB error estimate")
```

From these results, we can see that our model converges with about 50 trees.
The final model resulted in OOB error estimate of `r round(tail(model$finalModel$err.rate[,1],1)*100,1)`% (`r round(tail(model$finalModel$err.rate[,1],1),3)` in the plot), corresponding to an accuracy of `r round(max(model$results[,1])*100,1)`%.

We also analyze the variables importance. From the plot below, we can see that there are really only seven relevante variables, which are also listed.

```{r varimp}
varImport <- model$finalModel$importance
plot(model$finalModel$importance,
     main="Variable importance", xlab="Variable index", ylab="Importance")
head(varImport[order(varImport, decreasing=TRUE),], 7)
```

#### Performance

Finally, we calculate our actual performance metrics on the testing set.

```{r metrics}
predTesting  <- predict(model, dataTesting)
cm <- confusionMatrix(predTesting, dataTesting$classe)
cm
```

The achieved accuracy is `r round(cm$overall[1]*100,1)`%, with a 95% confidence interval of (`r round(cm$overall[3]*100,1)`%, `r round(cm$overall[4]*100,1)`%), which precisely matches estimates from our OOB, with `r round(max(model$results[,1])*100,1)`% accuracy.


## Quiz

We are now ready to answer the quiz, with high expectations.

```{r quiz, results='hide'}
dataQuiz <- predict(preProc, dataQuiz)
predQuiz <- predict(model, dataQuiz)
predQuiz
```

And... Yay! We got 100% correct  :-)
